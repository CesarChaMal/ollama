https://klu.ai/glossary/ollama
https://ollama.com/search?q=&p=1

curl https://ollama.ai/install.sh -o install.sh
curl https://ollama.ai/install.sh | sh
curl -fsSL https://ollama.com/install.sh | sh

sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl stop ollama
sudo systemctl start ollama
sudo systemctl status ollama

# create ollama user
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama

# Create a service file in /etc/systemd/system/ollama.service:
------------------------
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3

[Install]
WantedBy=default.target
------------------------

# Start the service 
sudo systemctl daemon-reload
sudo systemctl enable ollama

# start ollama
sudo systemctl start ollama

# update
curl -fsSL https://ollama.com/install.sh | sh

# logs
journalctl -u ollama

# uninstall
sudo systemctl stop ollama
sudo systemctl disable ollama
sudo rm /etc/systemd/system/ollama.service
sudo rm $(which ollama)
sudo rm -r /usr/share/ollama
sudo userdel ollama
sudo groupdel ollama


ollama serve
sudo lsof -i :11434
command -v systemctl >/dev/null && sudo systemctl stop ollama

ollama pull llama2:7b
ollama stop llama2:7b
ollama rum llama2:7b
ollama run --gpu llama2:7b

ollama run qwen:0.5b
ollama run qwen:1.8b
ollama run qwen:4b
ollama run qwen:7b
ollama run qwen:14b
ollama run qwen:72b
ollama run stable-code
ollama run tinyllama
ollama run stablelm2
ollama run tinydolphin
ollama run stablelm2
ollama run stablelm2:zephyr
ollama run starcoder:1b
ollama run starcoder:3b
ollama run starcoder:7b
ollama run starcoder:15b
ollama run mistral
ollama run mistral:7b-instruct-q2_K
ollama run mistral:7b-text-q2_K
ollama run mistral:7b-instruct-q3_K_S
ollama run mistral:7b-text-q3_K_S
ollama run llama2:7b
ollama run llama2:13b
ollama run llama2
ollama run llava
ollama run codellama
ollama run deepseek-coder
ollama run deepseek-llm
ollama run meditron
ollama run orca
ollama run orca-mini
ollama run vicuna
ollama run nous-hermes
ollama run wizard-vicuna

ollama rm $(ollama list | grep starcoder | awk '{print $1}')

ls -ltra ~/.ollama
ls -ltra ~/.ollama/models
ls -ltra /usr/share/ollama/.ollama
ls -ltra /usr/share/ollama/.ollama/models/blobs/

apt-get install jq -y
pip install Pygments -y

#Use curl with jq: Once jq is inst

curl http://127.0.0.1:11434/
curl http://127.0.0.1:11434/ | jq .
curl http://127.0.0.1:11434/ | pygmentize -l console
xdg-open http://127.0.0.1:11434/

docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v ollama-webui:/app/backend/data --name ollama-webui --restart always ghcr.io/ollama-webui/ollama-webui:main

xdg-open http://localhost:3000/

-- deepseek-coder

>>> /show
Available Commands:
  /show info         Show details for this model
  /show license      Show model license
  /show modelfile    Show Modelfile for this model
  /show parameters   Show parameters for this model
  /show system       Show system message
  /show template     Show prompt template

>>> /show modelfile
# Modelfile generated by "ollama show"
# To build a new Modelfile based on this one, replace the FROM line with:
# FROM deepseek-coder:latest

FROM /usr/share/ollama/.ollama/models/blobs/sha256:d040cc18521592f70c199396aeaa44cdc40224079156dc09d4283d745d9dc5fd
TEMPLATE """{{ .System }}
### Instruction:
{{ .Prompt }}
### Response:
"""
SYSTEM """You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer."""

-- mistral

>>> /show
Available Commands:
  /show info         Show details for this model
  /show license      Show model license
  /show modelfile    Show Modelfile for this model
  /show parameters   Show parameters for this model
  /show system       Show system message
  /show template     Show prompt template

>>> /show modelfile
# Modelfile generated by "ollama show"
# To build a new Modelfile based on this one, replace the FROM line with:
# FROM mistral:latest

FROM /usr/share/ollama/.ollama/models/blobs/sha256:e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730
TEMPLATE """[INST] {{ .System }} {{ .Prompt }} [/INST]"""
PARAMETER stop "[INST]"
PARAMETER stop "[/INST]"
>>> Send a message (/? for help)

